[
  {
    "objectID": "PP.html",
    "href": "PP.html",
    "title": "Point process modelling inlabru",
    "section": "",
    "text": "The general idea is to fit a LGCP that includes (1) a spatial covariate (elevation in this case) and (2) an SPDE to account for unexplained spatial variation. Spatial covariates can be available in different formats. In the gorillas example, the elevation variable is available as SpatialPixelsDataFrame spatial-class object but we will convert it to a raster as this is in many data sets spatial covariates are read as raster data.\n\n\n\n\n\n\nNote\n\n\n\nNote that inlabru articles on the website have already been updated and thus, the material in here is very similar to the one found online. We will use the gorillas_sf data which contains an updated version of the variables in the original data set.\n\n\nFirs we will begin reading the data:\n\nlibrary(INLA)\nlibrary(sf)\nlibrary(inlabru)\nlibrary(stars)\nlibrary(terra)\nlibrary(ggplot2)\n# Data sets \n\ndata(gorillas_sf)\n\nnests &lt;- gorillas_sf$nests\nmesh &lt;- gorillas_sf$mesh\nboundary &lt;- gorillas_sf$boundary\ngcov &lt;- gorillas_sf_gcov()\n\n\nggplot() +\n  gg(mesh) +\n  geom_sf(data = boundary, alpha = 0.1, fill = \"blue\") +\n  geom_sf(data = nests) +\n  ggtitle(\"Points\")\n\n\n\n\nNow we define the model components. Recall for a LGCP (and point process in general) we need the covariates to be available everywhere. Thus, we can specify the following:\n\neval_spatial(raster, where=geometry,...)\nraster covariates can be of any class supported by eval_spatial. Note that eval_spatial would be called automatically for raster inputs:\n\ncomp3a &lt;- geometry ~ elevation(gcov$elevation, model = \"linear\")\nfit3a &lt;- lgcp(comp3a, nests, samplers = boundary, domain = list(geometry = mesh))\nfit3a$summary.fixed\n\nHowever, if it doesn’t auto detect it then you should call the eval_spatial:\n\ncomp3b &lt;- geometry ~ elevation(eval_spatial(data=gcov$elevation,where = .data.), model = \"linear\")\nfit3b &lt;- lgcp(comp3b, nests, samplers = boundary, domain = list(geometry = mesh))\nfit3b$summary.fixed\n\n                  mean           sd   0.025quant     0.5quant   0.975quant\nelevation  0.004194011 0.0002466676  0.003710552  0.004194011  0.004677471\nIntercept -3.817061673 0.4464031906 -4.691995849 -3.817061673 -2.942127497\n                  mode kld\nelevation  0.004194011   0\nIntercept -3.817061673   0\n\n\nHere, where = .data. is bascially telling where do we want to evaluate. In this case we look for the geometry column in the data input . so the same results could be obtained by calling directly the name of the column as:\n geometry ~ elevation(eval_spatial(data=gcov$elevation,where = geometry), model = \"linear\")\nSince we want to evaluate the covariate at arbitrary points in the survey region we want these values to be defined everywhere. So if the values of a raster are missing for certain evaluation points, inlabru will impute it with the nearest-available value.\n\nelevation_missing &lt;- gcov$elevation\nvalues(elevation_missing)[values(elevation_missing)==2008] &lt;- NA\nggplot()+geom_spatraster(data = elevation_missing)\n\nError in geom_spatraster(data = elevation_missing): could not find function \"geom_spatraster\"\n\ncomp3_missing &lt;- geometry ~ elevation(elevation_missing, model = \"linear\")\nfit3_miss &lt;- lgcp(comp3_missing, nests, samplers = boundary, domain = list(geometry = mesh))\n\nWarning in input_eval.bru_input(subcomp$input, data = lh$data, env = env, : Model input 'elevation_missing' for 'elevation' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\n\nWarning in input_eval.bru_input(component[[part]]$input, data, env = component$env, : Model input 'elevation_missing' for 'elevation' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\nfit3_miss$summary.fixed\n\n                  mean           sd   0.025quant     0.5quant   0.975quant\nelevation  0.004193971 0.0002466463  0.003710553  0.004193971  0.004677389\nIntercept -3.816986410 0.4463657581 -4.691847220 -3.816986410 -2.942125601\n                  mode kld\nelevation  0.004193971   0\nIntercept -3.816986410   0\n\n\nAlternatively, we could write a function that does this for us (we use bru_fill_missing to impute the missing values) :\n\nf.elev &lt;- function(raster,where) {\n  # Extract the values\n  v &lt;- eval_spatial(raster, where)\n  # Fill in missing values\n  if (any(is.na(v))) {\n    v &lt;- bru_fill_missing(raster, where, v)\n  }\n  return(v)\n}\n\ncomp3_in &lt;- geometry ~ elevation(f.elev(raster = elevation_missing,where = .data.), model = \"linear\")\nfit3_in &lt;- lgcp(comp3_in, nests, samplers = boundary, domain = list(geometry = mesh))\nfit3_in$summary.fixed\n\n                  mean           sd   0.025quant     0.5quant   0.975quant\nelevation  0.004193971 0.0002466463  0.003710553  0.004193971  0.004677389\nIntercept -3.816986410 0.4463657581 -4.691847220 -3.816986410 -2.942125601\n                  mode kld\nelevation  0.004193971   0\nIntercept -3.816986410   0\n\n\nLets fit now a LGCP including a continuous spatial effects and an elevation effect.\n\nmatern &lt;- inla.spde2.pcmatern(\n  mesh,\n  prior.sigma = c(0.1, 0.01),\n  prior.range = c(0.05, 0.01)\n)\n\ncomp_lgcp &lt;- geometry ~ Intercept(1)  +\n             elevation(gcov$elevation, model = \"linear\")+\n             field(geometry, model = matern)\n\nfit_lgcp &lt;- lgcp(comp_lgcp, nests, \n                 samplers = boundary, \n                 domain = list(geometry = mesh))\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that for a model with an intercept only we need to tell the correct size :\n geometry ~ 0 + Intercept(rep(1, nrow(.data.)), model = \"linear\")\n\n\nNow we can estimate the abundance by computed the integrated intensity \\(\\lambda_{int}\\) over the domain. To do so, we first obtain the integration points using the fm_int function which contains the integration weight values:\n\n\n\n\n\n\n\n\n\n\nLambda &lt;- predict(\n  fit_lgcp,\n  fm_int(mesh, boundary),\n  ~ sum(weight * exp(Intercept + elevation + field))\n)\nLambda\n\n      mean       sd   q0.025     q0.5   q0.975   median mean.mc_std_err\n1 678.3862 24.87518 627.7427 677.8863 737.2592 677.8863        2.487518\n  sd.mc_std_err\n1      1.964852\n\n\nGiven this values of \\(\\lambda_{int}\\) we can estimate the posterior abundance as follows:\n\nNest &lt;- predict(\n  fit_lgcp,\n  fm_int(mesh, boundary),\n  ~ data.frame(\n    N = 500:800,\n    dpois(500:800,\n      lambda = sum(weight * exp(Intercept + elevation + field))\n    )\n  )\n)\n\nggplot(data = Nest) +\n  geom_line(aes(x = N, y = mean, colour = \"Posterior\"))"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mesh settings",
    "section": "",
    "text": "Parameters are on the scale of spatial coordinates (e.g., if working in Easting and Northing, the parameters will be in km).\n\n\n\nmax.edge defines the maximum length of triangle edges. Lower values mean smaller triangles, resulting in a finer overall mesh.\nmax.edge can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nIn the outer mesh, max.edge can be very large, as we are not interested in fine-scale inference for this area. We also want this to be a large value to improve computational efficiency, as the number of nodes in the mesh is proportional to the running time of the model.\nIn the inner mesh, we need to have a fine enough resolution to capture the processes we are interested in. This means that we want the max.edge to be smaller than the spatial range across which we would expect our points to be correlated (SPDE range parameter).\n\nInitially, if we have no other prior knowledge about the SPDE range parameter, we can approximate it as 1/3 of the study area, but it should then be replaced once a posterior estimate is obtained. The max.edge should then be between 1/10 and 1/5 of this spatial range value.\n\nA mesh with a finer resolution will lead to a higher number of mesh nodes, resulting in a model with a longer running time. So, when choosing a value for max.edge, consider the size of the study area and look at the number of nodes in your resulting mesh.\n\n\n\n\n\nmin.angle defines the minimum angles at which triangles join.\nmin.angle can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nI believe changing min.angle has a similar effect on the overall mesh to changing max.edge, but with a less intuitive interpretation. I would therefore avoid specifying this parameter explicitly in mesh construction.\n\n\n\n\n\noffset defines the extension distance for the mesh boundaries.\noffset can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nAn outer boundary extension is required to buffer the Boundary Effect. The Boundary Effect is an increase in the SD of estimates near the boundary, due to a boundary condition imposed on the SPDE. In order to avoid this, we create a boundary extension, leaving this effect at the outer boundary, so it doesn’t impact our area of interest (within the inner boundary).\nA general rule is to use 1\\(\\times\\) the max.edge for the inner boundary offset and 5\\(\\times\\) the max.edge for the outer boundary offset.\nThe effectiveness of our outer boundary on buffering the Boundary Effect can be evaluated using the meshbuilder() tool (example below).\nIf negative, offset is interpreted as a factor relative to the approximate data diameter.\n\n\n\n\n\ncutoff defines the minimum allowed distance across which 2 mesh nodes are required. Nodes which are at most as far apart as this are replaced by a single vertex.\nIn other words, cutoff represents the minimum distance across which two vertices are required, and so works in tandem with max.edge to determine mesh resolution.\nThe main purpose of cutoff is to even-out the size of triangles in the mesh, so that all triangles in the area of interest (inside the inner mesh boundary) are a similar size.\n\nWhen using points as a base to build a mesh from, we can end up with clusters of small triangles around these points. Increasing cutoff smooths this out, so that all triangles in the inner mesh are a similar size. A cutoff value of max.edge/5 usually evens out clustering.\n\nDecreasing the cutoff value can help to better represent complex boundaries such as coastlines, as the number of triangles along complex edges is increased. A larger cutoff value may smooth over these edges, resulting in a simplified representation of the boundary.\n\n\n\n\n\nboundary can be used to include a polygon for the area of interest, and is used to create the inner mesh boundary.\nThere should be at least 1 ‘spatial range’ (i.e., posterior SPDE range parameter) between the boundary and any point. If this is not known, it can be initially approximated as 1/3 of the study area, but should then be replaced once a posterior estimate is obtained.\n\n\n\n\n\nlocs can be used to define point locations from which to build the initial triangulation nodes of the mesh.\nThis can result in clustering of mesh nodes around point locations, which if undesired, can be fixed using the cutoff parameter.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis information is obtained from inlabru teaching materials documents Git Repo\n\n\nFor more information about mesh construction visit https://rpubs.com/jafet089/8866871"
  },
  {
    "objectID": "about.html#mesh-parameters",
    "href": "about.html#mesh-parameters",
    "title": "Mesh settings",
    "section": "",
    "text": "Parameters are on the scale of spatial coordinates (e.g., if working in Easting and Northing, the parameters will be in km).\n\n\n\nmax.edge defines the maximum length of triangle edges. Lower values mean smaller triangles, resulting in a finer overall mesh.\nmax.edge can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nIn the outer mesh, max.edge can be very large, as we are not interested in fine-scale inference for this area. We also want this to be a large value to improve computational efficiency, as the number of nodes in the mesh is proportional to the running time of the model.\nIn the inner mesh, we need to have a fine enough resolution to capture the processes we are interested in. This means that we want the max.edge to be smaller than the spatial range across which we would expect our points to be correlated (SPDE range parameter).\n\nInitially, if we have no other prior knowledge about the SPDE range parameter, we can approximate it as 1/3 of the study area, but it should then be replaced once a posterior estimate is obtained. The max.edge should then be between 1/10 and 1/5 of this spatial range value.\n\nA mesh with a finer resolution will lead to a higher number of mesh nodes, resulting in a model with a longer running time. So, when choosing a value for max.edge, consider the size of the study area and look at the number of nodes in your resulting mesh.\n\n\n\n\n\nmin.angle defines the minimum angles at which triangles join.\nmin.angle can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nI believe changing min.angle has a similar effect on the overall mesh to changing max.edge, but with a less intuitive interpretation. I would therefore avoid specifying this parameter explicitly in mesh construction.\n\n\n\n\n\noffset defines the extension distance for the mesh boundaries.\noffset can have one or two input values.\n\nOne value would be used for a mesh with no outer boundary extension (not recommended).\nTwo values relate to the specification for the inner mesh (inside the inner mesh boundary) and outer mesh (between the inner and outer mesh boundaries).\n\nAn outer boundary extension is required to buffer the Boundary Effect. The Boundary Effect is an increase in the SD of estimates near the boundary, due to a boundary condition imposed on the SPDE. In order to avoid this, we create a boundary extension, leaving this effect at the outer boundary, so it doesn’t impact our area of interest (within the inner boundary).\nA general rule is to use 1\\(\\times\\) the max.edge for the inner boundary offset and 5\\(\\times\\) the max.edge for the outer boundary offset.\nThe effectiveness of our outer boundary on buffering the Boundary Effect can be evaluated using the meshbuilder() tool (example below).\nIf negative, offset is interpreted as a factor relative to the approximate data diameter.\n\n\n\n\n\ncutoff defines the minimum allowed distance across which 2 mesh nodes are required. Nodes which are at most as far apart as this are replaced by a single vertex.\nIn other words, cutoff represents the minimum distance across which two vertices are required, and so works in tandem with max.edge to determine mesh resolution.\nThe main purpose of cutoff is to even-out the size of triangles in the mesh, so that all triangles in the area of interest (inside the inner mesh boundary) are a similar size.\n\nWhen using points as a base to build a mesh from, we can end up with clusters of small triangles around these points. Increasing cutoff smooths this out, so that all triangles in the inner mesh are a similar size. A cutoff value of max.edge/5 usually evens out clustering.\n\nDecreasing the cutoff value can help to better represent complex boundaries such as coastlines, as the number of triangles along complex edges is increased. A larger cutoff value may smooth over these edges, resulting in a simplified representation of the boundary.\n\n\n\n\n\nboundary can be used to include a polygon for the area of interest, and is used to create the inner mesh boundary.\nThere should be at least 1 ‘spatial range’ (i.e., posterior SPDE range parameter) between the boundary and any point. If this is not known, it can be initially approximated as 1/3 of the study area, but should then be replaced once a posterior estimate is obtained.\n\n\n\n\n\nlocs can be used to define point locations from which to build the initial triangulation nodes of the mesh.\nThis can result in clustering of mesh nodes around point locations, which if undesired, can be fixed using the cutoff parameter.\n\n\n\n\n\n\n\nImportant\n\n\n\nThis information is obtained from inlabru teaching materials documents Git Repo\n\n\nFor more information about mesh construction visit https://rpubs.com/jafet089/8866871"
  },
  {
    "objectID": "about.html#footnotes",
    "href": "about.html#footnotes",
    "title": "Mesh settings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis vignette was created based on the older inla.mesh.2d() function and will be updated to fmesher eventually.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tidying and Wrangling spatial data",
    "section": "",
    "text": "Ecological studies usually involve the analysis of data that are geographically referenced. Over the last decade, different packages have been developed to handle geographic data in R with the sp package (now not longer supported) being for many years one of the key geospatial R packages for handling spatial data. However, new technologies have shown an important increase in the accuracy of georeferencing, leading to a more precise coordinate representation. In consequence, the code in which coordinate reference systems (CRS) are written has been updated. To have a better understanding of what these changes are and a bit of the history behind, I recommend the following reading:\nhttps://psfaculty.plantsciences.ucdavis.edu/plant/additionaltopics_datumwarning.pdf\nIn summary, recent changes in GDAL and PROJ packages (and in consequence any package that depended on these ones) do not longer support using proj4strings to represent a CRS because it cannot hold all the details of a projection. Intead, the current approach to represent a CRS is with the WKT2 strings, a standardized text-based format developed by the Open Geospatial Consortium (OGC). However, coding the WKT2 manually can be difficult due to the complexity involved in the CRS specification. A common aproach to address this is to specify the EPSG code (European Petroleum Survey Group) which is a uniform mapping system that identifies each coordinate refence system and projections. As a consequence of all of these changes, the OGC developed the sf package as standardized way to encode spatial data that has replaced the sp and rgeos packages. The sf has gained a lot of support and different resources and tutorials have become available. Here are some tutorials you might find useful to get you started with the sf library.\nThe sf package provides a more convenient and flexible framework for handling spatial data by defining more intuitive spatial data-structures compared to sp. In the next sections I will go throught some examples for handling spatial objects, CRS and projections using sf and their implementation with inlabru."
  },
  {
    "objectID": "index.html#managing-crs",
    "href": "index.html#managing-crs",
    "title": "Tidying and Wrangling spatial data",
    "section": "\n1.1 Managing CRS",
    "text": "1.1 Managing CRS\nNotice that the default units are in meters, to change to say Km, we simply transform the crs accordingly:\n\nEngland_border = st_transform(England_border,gsub(\"units=m\",\"units=km\",st_crs(England_border)$proj4string)) \n\nst_crs(England_border)$units\n\n[1] \"km\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe can remove all small islands for e.g. by computing the different polygon and sort them out based on their area.\n\nCodeEngland_border_mainland &lt;- England_border %&gt;% st_cast(\"POLYGON\") %&gt;%\n  dplyr::mutate(area = st_area(.)) %&gt;%\n  arrange(desc(area)) |&gt;\n  slice(1) \n\nggplot()+geom_sf(data=England_border_mainland)\n\n\n\n\n\n\n\n\n\nNow lets load some occurrence data an set everything on the correct CRS. First we load the presence-absence records for the speckled wood butterfly (Pararge aegeria) and convert this to an sf class object while assigning the correct crs.\n\noccurrences &lt;- read.csv(\"P_Aegeria_WCBS.csv\" ,h=T)\n\noccurrences_sf &lt;- occurrences %&gt;% st_as_sf(coords = c(\"X\", \"Y\"),\n                           crs = st_crs(England_border))\n\n\n\n\n\n\n\nNote\n\n\n\nWe know that these data are already in the nothings and eastings. However, your data might be in longitude and latitude coordinates. Thus, you can set the original crs when converting it to an sf class and the use the st_stranform to transform it to a different CRS.\nmy_data %&gt;% st_as_sf(coords = c(\"longitude\", \"latitude\"),\n                 crs = \"+proj=longlat +datum=WGS84\") %&gt;% \n  st_transform(st_crs(England_border))\n\n\nFor the elevation raster, we can use the project function to assign the correct crs and the we can mask and crop it to the area we are interested in.\n\nelevation_England &lt;- uk_elevation %&gt;% \n  project(crs(England_border_mainland)) %&gt;%\n  mask(England_border_mainland)%&gt;%\n  crop(England_border_mainland) %&gt;% terra::scale()"
  },
  {
    "objectID": "index.html#visualization",
    "href": "index.html#visualization",
    "title": "Tidying and Wrangling spatial data",
    "section": "\n1.2 Visualization",
    "text": "1.2 Visualization\nIf everything is fine you should be able to overlay the different layers in a single plot. Both sf and tidyterra integrates easily with ggplot2. So we can call geom_sf and geom_spatraster to add a layer to our base ggplot:\n\nCodeggplot() + tidyterra::geom_spatraster(data=elevation_England) + \n  scale_fill_viridis(name=\"Elevation\")+ \n  geom_sf(data= occurrences_sf, aes(colour= factor(y)),size=0.9)+\n  scale_color_manual(name=\"Occupancy \\n status\",\n                     values=c(\"skyblue\",\"orange\"),\n                     labels=c(\"Absence\",\"Presence\"))"
  },
  {
    "objectID": "index.html#logistic-regression-example",
    "href": "index.html#logistic-regression-example",
    "title": "Tidying and Wrangling spatial data",
    "section": "\n2.1 Logistic regression example",
    "text": "2.1 Logistic regression example\nWe will fit a logistic regression to the speckled wood occurrence data to illustrate a straight forward implementation of a spatial model using spatial covariates. The model likelihood is given by:\n\\[\ny_i \\sim \\mathrm{Bernoulli}(\\psi_i)\n\\]\nWhere the probability of presence \\(\\psi\\) is defined as a logit-scaled linear predictor \\(\\eta\\):\n\\[\n\\mathrm{logit}(\\psi_i) = \\eta_i = \\beta_0 +\\beta_1 \\mbox{elevation}_i + \\omega_i\n\\]\nHere, \\(\\omega\\) can be defined as a i.i.d site-level random effects or as a continuous Gaussian spatial field with Matérn covariance. We can easily evaluate the elevation values at the locations where observations were made.\n\nOne option is to extract the raster values at target sf object coordinates and add this data to the input data frame.\n\n\noccurrences_sf &lt;- occurrences_sf %&gt;% \n  mutate(terra::extract(x = elevation_England,y=occurrences_sf))\n\n\nlibrary(inlabru)\nlibrary(INLA)\n\ncmp_model1 &lt;- ~ -1 + \n    Intercept(1,mean.linear=0, prec.linear=1) + \n    elev(GBR_elv_msk, model = \"linear\",mean.linear=0, prec.linear=1) +\n    site_raneff(ID, model = \"iid\") \n  \n  \nlik_model1 &lt;- like(\"binomial\",\n                     formula = y ~ .,\n                     data = occurrences_sf,\n                     Ntrials = 1,\n                     control.family = list(link = \"logit\"))\n  \n  \nmodel_1  &lt;- bru(cmp_model1 , lik_model1)\n\n\n\n\n\n\n\n\nmean\n      sd\n      0.025quant\n      0.975quant\n      mode\n    \n\n\n0.869\n0.089\n0.694\n1.043\n0.869\n\n\n−0.363\n0.117\n−0.593\n−0.134\n−0.363\n\n\n\n\n\n\nWhile appending the covariate values to the input data is straightforward, there is an even more convenient way in inlabru. But first, lets fit a explicit spatial model. To do so we will look at the new fmesher library to help us build the mesh."
  },
  {
    "objectID": "index.html#building-a-mesh-with-fmesher",
    "href": "index.html#building-a-mesh-with-fmesher",
    "title": "Tidying and Wrangling spatial data",
    "section": "\n3.1 Building a mesh with fmesher\n",
    "text": "3.1 Building a mesh with fmesher\n\nIf we don’t have a good prior understanding of what our spatial range might be, we can initially approximate it with 1/3 of the study range. Then, we create the value max_edge, which could be between 1/10 and 1/5 of this value. The max.edge is max_edge for the inner mesh, and 5\\(\\times\\) max_edge for the outer mesh. The cutoff is max_edge/5. The offset is max_edge for the inner boundary, and 5\\(\\times\\) max_edge for the outer boundary. In the next example I illustrate how the mesh can be created using a non-convex hull or using the England sf boundary we defined earlier.\n\nlibrary(fmesher)\n\nboundary0 = inla.nonconvex.hull(st_coordinates(occurrences_sf),convex = -0.1)\nmax.edge = 20\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(1,4)*max.edge,\n                          cutoff = 15)\nmesh_1 = fm_mesh_2d(boundary = England_border_mainland,\n                    max.edge = c(1, 4) * max.edge,\n                    cutoff = 15,\n                  crs= crs(England_border_mainland))"
  }
]